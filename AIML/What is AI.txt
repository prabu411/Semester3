What is AI?

Artificial Intelligence builds agents that perceive reason act to achieve goals.

Preception:Vison,speech,sensors.

Reasoning & Learning: Search,planning,ML

Action:robitics,control

Application:healthcare(diagnolsis),finance(fraud),maps(routing),assistant(NLP),games(chess/Go),robotics(drones).

Classification of AI system by Environment:

In AI, the environment is Everything external to
the agent that it interacts with. Agents perceive
the enviroinment through sensors and act usig using actuators.

1.Observable:

Fully Observable:
The agent's sensor can access the complete state of the Environment at each Point in time.

Example:Chess or tic-tac-toe (the whole board is visible).


Partially Observable: The agent only perceives part of the state; the environment may be hidden or noisy.

Example: Self-driving cars (canâ€™t see around corners, sensors may fail

2.Agents:
Single-Agent:Only one agent is operating,no competition or cooperation.

Example:Crossword puzzle Solver.

Multi-Agent:Multiple agent exist,action affect each other.

Example:

Cooperative: robots moving boxes together.
Competitive: chess, poker, online auctions.

3. Determinism

Deterministic: The next state is completely determined by the current state and action.
Â 
Example: Mathematical puzzles, solving a maze.

Stochastic (Non-deterministic): Outcomes are uncertain due to randomness or unknown factors.

Example: Delivery drones (affected by wind/weather).


4. Episodic vs Sequential

Episodic: Agentâ€™s experience is divided into independent episodes. Each decision does not depend on previous ones.

Example: Image recognition (classifying one image at a time).

Sequential: Current decisions affect future actions.

Example: Chess, driving a car (past choices matter for future).


5. Static vs Dynamic

Static: The environment does not change while the agent is deciding.

Example: Crossword puzzles.

Dynamic: Environment may change during computation.

Example: Stock market trading, real-time games.

Semi-dynamic: Environment doesnâ€™t change, but performance score does.

Example: Timed exam (questions stay same, but score reduces if you take too long).

6. Discrete vs Continuous

Discrete: States, actions, and perceptions are distinct and countable.

Example: Chess (finite moves).

Continuous: States, time, and actions are real-valued, infinite.

Example: Driving a car (continuous speed, steering angles, time).

7. Known vs Unknown

Known: Agent knows the rules, transitions, and effects of actions.

Example: Chess (rules are fixed).

Unknown: Agent must learn the effects of actions through interaction.

Example: Learning to play a new video game without rules provided

| Property        | Type 1        | Type 2     | Example                      |
| --------------- | ------------- | ---------- | ---------------------------- |
| **Observable**  | Fully         | Partially  | Chess vs Self-driving        |
| **Agents**      | Single        | Multi      | Puzzle vs Chess              |
| **Determinism** | Deterministic | Stochastic | Maze vs Drone                |
| **Episodes**    | Episodic      | Sequential | Image recognition vs Driving |
| **Change**      | Static        | Dynamic    | Crossword vs Stock market    |
| **State**       | Discrete      | Continuous | Chess vs Driving             |
| **Knowledge**   | Known         | Unknown    | Chess vs New game

Â          |

Types of Intelligence:

1.simple reflex Agent
2.Model based reflex Agents
3.Goal based Agents
4.Utility based Agents
5.Learning Agents.

3)Problem-Solving Agents & Search:

Great question, GaneshPrabu! Let's break down **generic tree search** and **generic graph search**â€”two foundational algorithms in AI and computer scienceâ€”especially useful in pathfinding, decision-making, and problem-solving.

---

## ğŸŒ³ Generic Tree Search

### ğŸ” What it is:
A **generic tree search** explores a problem space structured like a tree. It assumes **no repeated states**, meaning each node is visited only once.

### ğŸ§  How it works:
- Starts at the **root node** (initial state).
- Expands nodes by generating **successors**.
- Uses a **queue (fringe)** to keep track of nodes to explore.
- Applies a strategy (e.g., DFS, BFS, UCS, A*) to decide which node to expand next.

### âš ï¸ Limitations:
- Doesnâ€™t check for **repeated states**, so it can revisit the same state multiple times if the problem space is cyclic.
- Can be inefficient or even infinite in cyclic graphs.

---

## ğŸŒ Generic Graph Search

### ğŸ” What it is:
A **generic graph search** is an enhancement of tree search that **avoids revisiting states** by keeping track of explored nodes.

### ğŸ§  How it works:
- Same basic structure as tree search.
- Adds an **explored set** to record visited states.
- Before expanding a node, it checks if the state has already been explored.
- Prevents cycles and redundant work.

### âœ… Advantages:
- More efficient in cyclic or large graphs.
- Guarantees **completeness** (if a solution exists, it will find it).
- Avoids infinite loops.

---

## ğŸ” Comparison Table

| Feature                | Tree Search                     | Graph Search                    |
|------------------------|----------------------------------|----------------------------------|
| Repeated State Check   | âŒ No                            | âœ… Yes                           |
| Explored Set           | âŒ Not used                      | âœ… Used                          |
| Suitable for Cycles    | âŒ No                            | âœ… Yes                           |
| Memory Usage           | ğŸŸ¡ Lower                         | ğŸ”µ Higher                        |
| Completeness           | ğŸŸ¡ Depends on strategy           | âœ… Guaranteed (with proper strategy) |

---

## ğŸ§ª Example Use Case

Imagine solving a maze:
- **Tree search** might keep re-entering the same dead-end.
- **Graph search** remembers where itâ€™s been and avoids wasting time.

---

If you're prepping for exams or coding interviews, I can walk you through BFS, DFS, UCS, or A* in both tree and graph versions with examples. Want to dive deeper into one of those?


Input: problem
Frontier â† {Node(state=initial, parent=âˆ…, action=âˆ…, g=0)}
Loop:
Â  if Frontier empty â†’ return failure
Â  n â† select and remove a node from Frontier (strategy decides which)
Â  if Goal(n.state) â†’ return solution (trace parents)
Â  for each action a in A(n.state):
Â      s' â† Result(n.state, a); g' â† n.g + cost(n.state,a,s')
Â      add Node(s', parent=n, action=a, g=g') to Frontier


Input: problem
Frontier â† priority container as per strategy
Explored â† âˆ…
Insert initial node into Frontier
Loop:
Â  if Frontier empty â†’ failure
Â  n â† pop(Frontier)
Â  if Goal(n.state) â†’ return solution
Â  if n.state âˆ‰ Explored:
Â      add n.state to Explored
Â      for each action a:
Â          s' â† Result(...); g' â† n.g + cost(...)
Â          if s' âˆ‰ Explored and not in Frontier with lower g:
Â              push/update s' into Frontier


4)Uniformed search:
no